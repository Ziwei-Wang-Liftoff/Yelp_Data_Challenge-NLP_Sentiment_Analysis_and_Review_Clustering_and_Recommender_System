{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp_Data_Challenge - Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main tasks\n",
    "\n",
    "1. Data preprocessing\n",
    "    - 1.1 Define feature variables\n",
    "    - 1.2 Define target variable\n",
    "    - 1.3 Create training dataset and test dataset\n",
    "    - 1.4 Get NLP representation of the documents\n",
    "2. Cluster reviews with KMeans\n",
    "    - 2.1 Fit k-means clustering with the training vectors and apply it on all the data\n",
    "    - 2.2 Make predictions on all data\n",
    "    - 2.3 Inspect the centroids\n",
    "    - 2.4 Try using different k (clusters)\n",
    "3. Cluster all the reviews of the most reviewed restaurant\n",
    "    - 3.1 Vectorize the text feature\n",
    "    - 3.2 Define target variable\n",
    "    - 3.3 Create train and test datasets\n",
    "    - 3.4 Get NLP representation of the documents\n",
    "    - 3.5 Cluster reviews with KMeans\n",
    "4. Other user cases of clustering\n",
    "    - 4.1 Different distance/similarity metrics for clusterings\n",
    "    - 4.2 Cluster restaurants by category information\n",
    "    - 4.3 Cluster restaurants by restaurant names\n",
    "    - 4.4 Cluster restaurants by tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "plt.style.use(\"ggplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset/last_2_years_restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>avg_stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>['Cajun/Creole', 'Steakhouses', 'Restaurants']</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-03-31</td>\n",
       "      <td>0</td>\n",
       "      <td>6SgvNWJltnZhW7duJgZ42w</td>\n",
       "      <td>5</td>\n",
       "      <td>This is mine and my fiancé's favorite steakhou...</td>\n",
       "      <td>0</td>\n",
       "      <td>oFyOUOeGTRZhFPF9uTqrTQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Delmonico Steakhouse</td>\n",
       "      <td>['Cajun/Creole', 'Steakhouses', 'Restaurants']</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-02-10</td>\n",
       "      <td>0</td>\n",
       "      <td>UxFpgng8dPMWOj99653k5Q</td>\n",
       "      <td>5</td>\n",
       "      <td>Truly Fantastic!  Best Steak ever. Service was...</td>\n",
       "      <td>0</td>\n",
       "      <td>aVOGlN9fZ-BXcbtj6dbf0g</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   name                                      categories  \\\n",
       "0  Delmonico Steakhouse  ['Cajun/Creole', 'Steakhouses', 'Restaurants']   \n",
       "1  Delmonico Steakhouse  ['Cajun/Creole', 'Steakhouses', 'Restaurants']   \n",
       "\n",
       "   avg_stars  cool        date  funny               review_id  stars  \\\n",
       "0        4.0     0  2016-03-31      0  6SgvNWJltnZhW7duJgZ42w      5   \n",
       "1        4.0     0  2016-02-10      0  UxFpgng8dPMWOj99653k5Q      5   \n",
       "\n",
       "                                                text  useful  \\\n",
       "0  This is mine and my fiancé's favorite steakhou...       0   \n",
       "1  Truly Fantastic!  Best Steak ever. Service was...       0   \n",
       "\n",
       "                  user_id  \n",
       "0  oFyOUOeGTRZhFPF9uTqrTQ  \n",
       "1  aVOGlN9fZ-BXcbtj6dbf0g  "
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Filter positive reviews "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here I am only interested in perfect (5 stars) rating reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_positive = df[df['stars'] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210559"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define feature variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here feautre variable is the text of the review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents\"\n",
    "documents = df_positive['text'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210559\n"
     ]
    }
   ],
   "source": [
    "print(len(documents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Create training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: documents\n",
    "# Y: targets\n",
    "# Now split the data to training set 80% and test set 20%\n",
    "documents_train, documents_test = train_test_split(documents, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168447, 42112)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents_train), len(documents_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Get NLP representation of the documents\n",
    "\n",
    "#### Fit TfidfVectorizer with training data only, then tranform all the data to tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer, and name it vectorizer\n",
    "# choose a reasonable max_features, e.g. 1000 to fast the computation speed\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with your training data\n",
    "vectors_train = vectorizer.fit_transform(documents_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(168447, 1000)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocab of your tfidf\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to transform all the reviews\n",
    "vectors_documents = vectorizer.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Cluster reviews with KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Fit k-means clustering with the training vectors and apply it on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=8, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans()\n",
    "\n",
    "kmeans.fit(vectors_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Make predictions on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "assigned_cluster = kmeans.predict(vectors_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Inspect the centroids\n",
    "\n",
    "- Description: To find out what \"topics\" Kmeans has discovered we must inspect the centroids. Print out the centroids of the Kmeans clustering. These centroids are simply a bunch of vectors.  To make sense of them we need to map these vectors back into our 'word space'.  Think of each feature/dimension of the centroid vector as representing the \"average\" review or the average occurances of words for that cluster.\n",
    "- Solution: Find the top 10 features (words) within each cluster. \n",
    "- Steps: \n",
    "    - (1) Sort each centroid vector to find the top 10 features \n",
    "    - (2) Go back to our vectorizer object to find out what words each of these features corresponds to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clusters:(8, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Default of kmeans uses 8 clusters\n",
    "print ('number of clusters:' + str(kmeans.cluster_centers_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 features for each cluster:\n",
      "0: burger, fries, burgers, good, great, place, cheese, best, shake, food\n",
      "1: food, good, place, best, vegas, amazing, delicious, time, service, just\n",
      "2: excellent, service, food, great, place, good, vegas, definitely, restaurant, best\n",
      "3: love, place, food, great, good, service, amazing, best, friendly, staff\n",
      "4: pizza, great, crust, place, good, best, vegas, cheese, service, delicious\n",
      "5: great, food, service, place, amazing, good, awesome, friendly, staff, definitely\n",
      "6: sushi, place, roll, rolls, great, fresh, ayce, service, best, fish\n",
      "7: chicken, fried, good, food, rice, place, delicious, great, ordered, amazing\n"
     ]
    }
   ],
   "source": [
    "# print top 10 words of each cluster centers\n",
    "# step (1) Sort each centroid vector to find the top 10 features\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-11:-1]\n",
    "print(\"top 10 features for each cluster:\")\n",
    "# step (2) Go back to our vectorizer object to find out what words each of these features corresponds to\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(words[i] for i in centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We will try different k, because:\n",
    "    - Using eight clusters (default setting in kmeans), I found that several clusters are kind of similar to each other, such as in Cluster 0 and 7 might signify fast food restaurants. \n",
    "    - The rest of clusters have some significant meanings such as in Cluster 6, it mainly tell about Japanese restaurants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Try using different k (clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does the top features change after using 5 clusters?\n",
    "- Using five clusters, the difference among clusters stands out more significant than using eight clusters. Each cluster now has an unique topic, such as Cluster 0 is surrounding with the topic of chicken, Cluster 2 is relating to Japanese food, Cluster 3 is relating to the pizza, and Cluster 4 is mainly about service aspect in vegas.\n",
    "- However, the top features using five clusters seem to be highly overlapped with the default method. In fact, it's a good strategy to narrow down overlapped clusters into denser clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 features for each cluster:\n",
      "0: good,food,really,place,service,great,nice,love,chicken,time\n",
      "1: place,food,best,vegas,delicious,amazing,time,love,ve,just\n",
      "2: sushi,place,roll,rolls,great,fresh,ayce,service,best,fish\n",
      "3: pizza,great,place,crust,good,best,love,service,vegas,cheese\n",
      "4: great,food,service,place,amazing,awesome,friendly,excellent,staff,definitely\n"
     ]
    }
   ],
   "source": [
    "# Find the top 10 features for each cluster.\n",
    "kmeans = KMeans(n_clusters = 5)\n",
    "kmeans.fit(vectors_train)\n",
    "assigned_cluster = kmeans.predict(vectors_documents)\n",
    "\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-11:-1]\n",
    "print(\"top 10 features for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \",\".join(words[i] for i in centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the rating and review of a random sample of the reviews assigned to each cluster to get a sense of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0:\n",
      "    My friends and I come here every Friday! It is our tradition. :) We love Sushi Kaya for many, many reasons. Their sushi is so fresh and cold. We usually do all-you-can-eat, and start off with miso soup and seaweed salad. I love their spicy tuna, sashimi, yellowtail, and albacore. They have a nice fish-to-rice too ratio since some places give way too much rice! I love their mochi as a dessert. The service is usually great every time. It doesn't take long when we order for our AYCE sushi.\n",
      "\n",
      "This place gets packed on the weekends, and for good reason! I would wait the 20 minutes or make it easier for yourself, and call ahead of time to make reservations.\n",
      "cluster 1:\n",
      "    Oh how I miss Hawaii after coming here! If you're looking for good and cheap food, this is the place to be. We ordered a furikake chicken and that was more than enough for two people. We had a ton of leftover. The chicken has so much flavor! We also ordered a half order of avocado poke and it was just enough for me. The poke was good but didn't taste as fresh. Maybe I ordered the wrong flavor. I definitely want to try the shoyu or sesame poke next time. Anyways this place is delicious and gives huge portions so you can't go wrong with that.\n",
      "cluster 2:\n",
      "    Salad was amazing. The gyro had an excellent combination and balance. I could eat here every day\n",
      "cluster 3:\n",
      "    Awesome to finally have a Robertos on this side of shit\n",
      "Summerlin.   The downfall is the weekends, uber crowded, ordered 1 burrito and it took about 30 minutes.  Dining room was packed for people waiting on To go orders,  drive through was packed and people were coming in from the drive through inside asking where there orders were.  Long story short, good food, be prepared to wait.  I will count this as a new weekend, everyone wants to get some Robertos and a apartment community is next to it.\n",
      "cluster 4:\n",
      "    I was so excited to eat here. As I enjoy the clam chowder very much. However, it was not the case this time. \n",
      "\n",
      "The clam chowder was soooooooo bland. I don't know if it was sitting out too long. We ordered the crab cakes, too. What the heck?! This time they were so slim and small (see my pic)- sheesh, $13! They were plumper the last time. \n",
      "\n",
      "My husband was looking forward to the diablo pasta... no longer available so he ordered gumbo. He said it was okay. \n",
      "\n",
      "\n",
      "We will check out other recommended places in the comments and will NOT be returning here. That's too bad.\n"
     ]
    }
   ],
   "source": [
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = np.arange(0, vectors_documents.shape[0])[assigned_cluster==i]\n",
    "    sample_reviews = np.random.choice(cluster, 1, replace=False)\n",
    "    print(\"cluster %d:\" % i)\n",
    "    for review in sample_reviews:\n",
    "        print(\"    %s\" % df.loc[review]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cluster all the reviews of the most reviewed restaurant\n",
    "- 3.1 Vectorize the text feature\n",
    "- 3.2 Define the target variable\n",
    "- 3.3 Create train and test datasets\n",
    "- 3.4 Get NLP representation of the documents\n",
    "- 3.5 Cluster reviews with KMean\n",
    "\n",
    "#### Let's find the most reviewed restaurant and analyze its reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hash House A Go Go'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the business who got most reviews, get your filtered df, name it df_top_restaurant\n",
    "df_top_restaurant = df['name'].value_counts().index[0]\n",
    "df_top_restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We can also load restaurant profile information from the business dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>name</th>\n",
       "      <th>categories</th>\n",
       "      <th>avg_stars</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32737</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-22</td>\n",
       "      <td>0</td>\n",
       "      <td>psGDwACpn7tFmWm36865fA</td>\n",
       "      <td>4</td>\n",
       "      <td>There isn't much here for vegetarians, but I h...</td>\n",
       "      <td>0</td>\n",
       "      <td>Y76nS3L426UCz7N_1pUfUQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32738</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>0</td>\n",
       "      <td>ZY0ym6jDPXCnyzyRKSVTHg</td>\n",
       "      <td>4</td>\n",
       "      <td>Visiting Las Vegas again, and decided to stop ...</td>\n",
       "      <td>0</td>\n",
       "      <td>SeHCNZeTtVvL1HmKFOLSkQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32739</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-13</td>\n",
       "      <td>0</td>\n",
       "      <td>vPFRrO6k6ynH-CgGKJLpPQ</td>\n",
       "      <td>5</td>\n",
       "      <td>This place is as crazy as Las Vegas.  The twis...</td>\n",
       "      <td>0</td>\n",
       "      <td>SvpxzDdYOrrI9ntolyNSxQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32740</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-08-26</td>\n",
       "      <td>0</td>\n",
       "      <td>DOZWVKN2n4CAp7mtkhxiaw</td>\n",
       "      <td>1</td>\n",
       "      <td>I've eaten at Hash House A Go Go on the strip ...</td>\n",
       "      <td>0</td>\n",
       "      <td>Io0qqdu_PyKfkr8d7F19mg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32741</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-10-16</td>\n",
       "      <td>0</td>\n",
       "      <td>-UGGkrLKjWMdW2N9l2rb2Q</td>\n",
       "      <td>4</td>\n",
       "      <td>We were told that this was a good place for br...</td>\n",
       "      <td>0</td>\n",
       "      <td>JrILFVrSIRIacx2qTy5tiA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32742</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>0</td>\n",
       "      <td>VvPH04YYZ8RcOimJdZXU7A</td>\n",
       "      <td>4</td>\n",
       "      <td>EDC food. The chicken (2 breasts or thighs? Ca...</td>\n",
       "      <td>0</td>\n",
       "      <td>qyPBg6aUIAM83vbkNJCtSQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>32743</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-29</td>\n",
       "      <td>0</td>\n",
       "      <td>EdCoN1v8Tv7CtSRe5WLyNg</td>\n",
       "      <td>5</td>\n",
       "      <td>We went there after checking Yelp during our t...</td>\n",
       "      <td>0</td>\n",
       "      <td>PdiutioUdu9q8VhtHdzpVQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32744</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-19</td>\n",
       "      <td>0</td>\n",
       "      <td>uGDn7km6sXBQ8NlAC3chhg</td>\n",
       "      <td>5</td>\n",
       "      <td>Brunch was amazing! I mean the banana French t...</td>\n",
       "      <td>0</td>\n",
       "      <td>Eq_3Wq22Xjw2mxVln-NALw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>32745</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-23</td>\n",
       "      <td>0</td>\n",
       "      <td>umgRwr9PbF0xOM8p5H4Waw</td>\n",
       "      <td>2</td>\n",
       "      <td>I felt the service was not stellar at all. Wai...</td>\n",
       "      <td>0</td>\n",
       "      <td>Af2xB-Sfv-r0kdwl_FbGzg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>32746</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-30</td>\n",
       "      <td>0</td>\n",
       "      <td>uDf1xM8e9BzwtDwN6rb7IQ</td>\n",
       "      <td>4</td>\n",
       "      <td>I forget what our dish was called but it was t...</td>\n",
       "      <td>0</td>\n",
       "      <td>QpGBJKgosHPz7MBz95NGbA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>32747</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>0</td>\n",
       "      <td>qQwv5cLJsLHUwcSB6hjVhg</td>\n",
       "      <td>5</td>\n",
       "      <td>This place was very very yummy!! The plates ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>wJkbfSla1nx2Zdg4O_Ebng</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>32748</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>uEHLHXmBXs6UxZ89Pgck4w</td>\n",
       "      <td>4</td>\n",
       "      <td>Food is yummy, they give you a GREAT BIG porti...</td>\n",
       "      <td>1</td>\n",
       "      <td>ysyvsKXwesbsE2tKZNZXYg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>32749</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-09-15</td>\n",
       "      <td>0</td>\n",
       "      <td>k4-JIiEMSgaNji5tzwBWCw</td>\n",
       "      <td>3</td>\n",
       "      <td>We both had bacon and eggs for breakfast. My w...</td>\n",
       "      <td>0</td>\n",
       "      <td>v51Ii6NupJ0emzMMlVRDsw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>32750</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-19</td>\n",
       "      <td>0</td>\n",
       "      <td>sCj_sNKLHpmNbmaXnJrKaQ</td>\n",
       "      <td>4</td>\n",
       "      <td>This is like the cheesecake factory of vegas f...</td>\n",
       "      <td>1</td>\n",
       "      <td>DjP2lo5wOngdUBNh1NCvbA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>32751</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-26</td>\n",
       "      <td>0</td>\n",
       "      <td>GyCXSnQnyZM-AutIrnTdnQ</td>\n",
       "      <td>4</td>\n",
       "      <td>Been here twice now, but needed the second tri...</td>\n",
       "      <td>0</td>\n",
       "      <td>-Fb4gB1z0OKuo42NOoX9pQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>32752</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-09</td>\n",
       "      <td>0</td>\n",
       "      <td>MMm6XGrpwUYG43vn5Oi_7g</td>\n",
       "      <td>5</td>\n",
       "      <td>You absolutely must try this place. It is deli...</td>\n",
       "      <td>0</td>\n",
       "      <td>d2i9c1NKB02hjxjslciYag</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>32753</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-02</td>\n",
       "      <td>0</td>\n",
       "      <td>ACkcFtFSpH8T8TR0T0cRgQ</td>\n",
       "      <td>3</td>\n",
       "      <td>Ended up here twice on the last trip to Vegas ...</td>\n",
       "      <td>0</td>\n",
       "      <td>OIiYnTwm9qODSfOV-HJ_WQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>32754</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-31</td>\n",
       "      <td>0</td>\n",
       "      <td>qhy64zd1s2MEhsC8Jk3Mpg</td>\n",
       "      <td>4</td>\n",
       "      <td>The food was so fresh and delicious! Our serve...</td>\n",
       "      <td>0</td>\n",
       "      <td>k_lO7ngmIgJmfreKITdbNA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>32755</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-18</td>\n",
       "      <td>0</td>\n",
       "      <td>29O5vxvulGLgt9Pf8OlBMA</td>\n",
       "      <td>1</td>\n",
       "      <td>if you are into not being served when it is sl...</td>\n",
       "      <td>0</td>\n",
       "      <td>Br176HwewDGjppTO5zYWCA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>32756</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-09</td>\n",
       "      <td>0</td>\n",
       "      <td>lvvg7cyQleKOiAtUBQBwGw</td>\n",
       "      <td>4</td>\n",
       "      <td>My partner and I decided to eat here because o...</td>\n",
       "      <td>0</td>\n",
       "      <td>qUlxA7qV79Vj0MHoddgcRw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32757</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-13</td>\n",
       "      <td>0</td>\n",
       "      <td>kzAFyixmaVxq_OVkUzEhEg</td>\n",
       "      <td>4</td>\n",
       "      <td>Loved this location and how it wasn't congeste...</td>\n",
       "      <td>1</td>\n",
       "      <td>SS3sFA9ksCT9bjocM3Wbug</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>32758</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-03-26</td>\n",
       "      <td>0</td>\n",
       "      <td>Hml9uVRWpxE_a3Ulaa4UOQ</td>\n",
       "      <td>1</td>\n",
       "      <td>HORRIBLE SERVICE. Came in around 10 o'clock on...</td>\n",
       "      <td>0</td>\n",
       "      <td>VHxLO6QhzOo_4uk7fvABrQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>32759</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>0</td>\n",
       "      <td>71_kOnOLW3rUDZC2m27KJQ</td>\n",
       "      <td>5</td>\n",
       "      <td>So this has always been one of my favorite pla...</td>\n",
       "      <td>1</td>\n",
       "      <td>5lhisOo609Mymwf9-4MOlA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>32760</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-06-05</td>\n",
       "      <td>0</td>\n",
       "      <td>KI4f3vqYXcKn8KVvfadm-g</td>\n",
       "      <td>5</td>\n",
       "      <td>This place is amazing! We were staying at The ...</td>\n",
       "      <td>0</td>\n",
       "      <td>4UaEmDPikjev0avWU7YCnA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>32761</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-28</td>\n",
       "      <td>0</td>\n",
       "      <td>CA8gYlcOOvkB9fn2PjLysw</td>\n",
       "      <td>1</td>\n",
       "      <td>The only reason I got food to go from here was...</td>\n",
       "      <td>0</td>\n",
       "      <td>AkgO_aR6-DZ9KZgd3P4BYA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>32762</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-03-17</td>\n",
       "      <td>0</td>\n",
       "      <td>Xy0Co_vfxVElRoC3ULtk8A</td>\n",
       "      <td>5</td>\n",
       "      <td>You could go fuckin HAM up in here !! We're do...</td>\n",
       "      <td>0</td>\n",
       "      <td>eofcCttybB9kINM9B2ZTEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>32763</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-12</td>\n",
       "      <td>0</td>\n",
       "      <td>i5N6ZXARhEc0X4J0eee6hQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Amazing Service, Realistic prices, Large Porti...</td>\n",
       "      <td>0</td>\n",
       "      <td>4yiDNExj3eQb_8KqKoPFDg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>32764</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-09-12</td>\n",
       "      <td>0</td>\n",
       "      <td>kA-4M-jJe1H6Eq5UuFQ5mA</td>\n",
       "      <td>4</td>\n",
       "      <td>Definitely not for the weak stomach, or faint ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3g3HVDsFKaftXKN17v8fHg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>32765</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-30</td>\n",
       "      <td>0</td>\n",
       "      <td>oRTFazyWTz3DxxGT4nhcdQ</td>\n",
       "      <td>5</td>\n",
       "      <td>This place is delicious! Portions are big and ...</td>\n",
       "      <td>0</td>\n",
       "      <td>y0cPGHo3C7B8V8L5nsBw-Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>32766</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (New)', 'Restaurants', 'Breakfast &amp;...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-11-16</td>\n",
       "      <td>0</td>\n",
       "      <td>tikz-MWqWmO4GkgKVBugwg</td>\n",
       "      <td>4</td>\n",
       "      <td>Ordered the corn beef hash and am completely s...</td>\n",
       "      <td>0</td>\n",
       "      <td>_1nOURpPlpbNYbjqn1dfWg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3590</th>\n",
       "      <td>446228</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-10-25</td>\n",
       "      <td>0</td>\n",
       "      <td>KyZ8g0au1wYUSix9LblTUg</td>\n",
       "      <td>5</td>\n",
       "      <td>Excellent food. Giant portions. We came for br...</td>\n",
       "      <td>0</td>\n",
       "      <td>tDceZUKT7RzyamgNZETKzg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3591</th>\n",
       "      <td>446229</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-12-30</td>\n",
       "      <td>0</td>\n",
       "      <td>yjK-33zpOwnuXH-P9Kugdw</td>\n",
       "      <td>1</td>\n",
       "      <td>Got food twice due to circumstances and wouldn...</td>\n",
       "      <td>0</td>\n",
       "      <td>Xef5qBsJI4OkucEiSS1gcg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3592</th>\n",
       "      <td>446230</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2</td>\n",
       "      <td>2016-04-19</td>\n",
       "      <td>1</td>\n",
       "      <td>EHEQo7c1jQtiKkcsVrGCMQ</td>\n",
       "      <td>5</td>\n",
       "      <td>Wow lots of choices and SO much food!  This is...</td>\n",
       "      <td>2</td>\n",
       "      <td>NQffx45eJaeqhFcMadKUQA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3593</th>\n",
       "      <td>446231</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-02-01</td>\n",
       "      <td>1</td>\n",
       "      <td>QaEAZ1SsWdQmo8xFh-cvNw</td>\n",
       "      <td>4</td>\n",
       "      <td>Hash House Agogo..\\nIf you have not tried this...</td>\n",
       "      <td>2</td>\n",
       "      <td>8hbMyONy7kQEVosswobVtQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3594</th>\n",
       "      <td>446232</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-12-04</td>\n",
       "      <td>1</td>\n",
       "      <td>s0EZjSzxY7zLvUHGQ6LZHg</td>\n",
       "      <td>5</td>\n",
       "      <td>Holy breakfast!\\n\\nThis place truly knows how ...</td>\n",
       "      <td>1</td>\n",
       "      <td>zjYg5B443x1vK-GLONLa7w</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3595</th>\n",
       "      <td>446233</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-20</td>\n",
       "      <td>0</td>\n",
       "      <td>TvqqOUAl7tpL_0nNRPV_Nw</td>\n",
       "      <td>4</td>\n",
       "      <td>Wow, Great Food and Service! Fantastic Breakfa...</td>\n",
       "      <td>0</td>\n",
       "      <td>mQ7Eis5jofSsnCvRkySchA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3596</th>\n",
       "      <td>446234</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-10-19</td>\n",
       "      <td>1</td>\n",
       "      <td>Y7NlEAjiCTUkqmJGEi4_OQ</td>\n",
       "      <td>1</td>\n",
       "      <td>Hash House A Go Go is a sickening exercise is ...</td>\n",
       "      <td>2</td>\n",
       "      <td>ntlvfPzc8eglqvk92iDIAw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3597</th>\n",
       "      <td>446235</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-01-04</td>\n",
       "      <td>0</td>\n",
       "      <td>yeZv5VMIGL-qOFiaJ88-Nw</td>\n",
       "      <td>2</td>\n",
       "      <td>So this location in the Rio is only open for b...</td>\n",
       "      <td>1</td>\n",
       "      <td>ajxohdcsKhRGFlEvHZDyTw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3598</th>\n",
       "      <td>446236</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-02</td>\n",
       "      <td>0</td>\n",
       "      <td>OWtZ0oI-Z0EmJXvp7dfYoQ</td>\n",
       "      <td>4</td>\n",
       "      <td>This place is well worth it you truly get your...</td>\n",
       "      <td>0</td>\n",
       "      <td>4iNP1z6jTIiRHYMKEzWjQw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3599</th>\n",
       "      <td>446237</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-03-20</td>\n",
       "      <td>0</td>\n",
       "      <td>mMGTTy5GEwlz4Sw3NTDAyg</td>\n",
       "      <td>2</td>\n",
       "      <td>The food was decent. Service was good. Huge po...</td>\n",
       "      <td>0</td>\n",
       "      <td>zz9rIM0FmlWYvAzr6dGwVQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3600</th>\n",
       "      <td>446238</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-04-22</td>\n",
       "      <td>0</td>\n",
       "      <td>Eliuz50LW8kus7ISGCBrBQ</td>\n",
       "      <td>2</td>\n",
       "      <td>i have been a big fan of HH for a long time.  ...</td>\n",
       "      <td>0</td>\n",
       "      <td>3xLPeHhf2kD4WHAfkABJIw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3601</th>\n",
       "      <td>446239</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-01-03</td>\n",
       "      <td>0</td>\n",
       "      <td>nmrV3fqlxyuYa0Ey7TsmOg</td>\n",
       "      <td>3</td>\n",
       "      <td>Met up with friends for breakfast on 1/2/2016....</td>\n",
       "      <td>0</td>\n",
       "      <td>JnIhH0aEgMB0cZNLV9ya8g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3602</th>\n",
       "      <td>446240</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-04-13</td>\n",
       "      <td>0</td>\n",
       "      <td>7u9p8XhyqNqJ7wMvCwthKA</td>\n",
       "      <td>3</td>\n",
       "      <td>It was just ok. Coffee is good but the hash (w...</td>\n",
       "      <td>0</td>\n",
       "      <td>D5Q_eKLy1yolh9n0drT40A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3603</th>\n",
       "      <td>446241</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-24</td>\n",
       "      <td>0</td>\n",
       "      <td>gLLMEdwRB7csDD4MOrNYzQ</td>\n",
       "      <td>1</td>\n",
       "      <td>It's a place of a lot food but no flavor so di...</td>\n",
       "      <td>0</td>\n",
       "      <td>JfJDvW0OESr2SRizhPNypA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3604</th>\n",
       "      <td>446242</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-02-19</td>\n",
       "      <td>0</td>\n",
       "      <td>_6IkhF03vyJ5t3ybIYsDVw</td>\n",
       "      <td>3</td>\n",
       "      <td>Our waitress HANA is the besssssssst! She is a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2t0XjHgF9SXwiAoUeDAXsg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3605</th>\n",
       "      <td>446243</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-11-07</td>\n",
       "      <td>0</td>\n",
       "      <td>2wZ_ZT0Ov_nV8Qg1ze_Mng</td>\n",
       "      <td>3</td>\n",
       "      <td>People keep raving about this place but I was ...</td>\n",
       "      <td>0</td>\n",
       "      <td>XpfS2pcAgOIz6Jlyv9nIcg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3606</th>\n",
       "      <td>446244</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-08-03</td>\n",
       "      <td>0</td>\n",
       "      <td>PifKrfL9fLpBmL5CSfJSag</td>\n",
       "      <td>5</td>\n",
       "      <td>when i first read reviews of hash house a go g...</td>\n",
       "      <td>0</td>\n",
       "      <td>TqPVG4ZJM_cPBGI0K8lIUQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3607</th>\n",
       "      <td>446245</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-09-19</td>\n",
       "      <td>0</td>\n",
       "      <td>Gzo97HLHvtQqjanqJU6YPg</td>\n",
       "      <td>5</td>\n",
       "      <td>Huge portions...our waiter looked like The guy...</td>\n",
       "      <td>0</td>\n",
       "      <td>k2Oh3tkq3-FOXo2klkyU0Q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3608</th>\n",
       "      <td>446246</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-09-10</td>\n",
       "      <td>0</td>\n",
       "      <td>i3YL5DSiJ6t4nURd8SH6ww</td>\n",
       "      <td>3</td>\n",
       "      <td>I had the basic breakfast was ok. The eggs wer...</td>\n",
       "      <td>0</td>\n",
       "      <td>WJPeI1ArWf40JLwfcqhmSA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>446247</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-11-15</td>\n",
       "      <td>0</td>\n",
       "      <td>Y5cDr8IGwYaFS2vHaUk1Aw</td>\n",
       "      <td>4</td>\n",
       "      <td>The food is good, and the portions are enormou...</td>\n",
       "      <td>0</td>\n",
       "      <td>hIOja3-46E73HgYakqoRuw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3610</th>\n",
       "      <td>446248</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-10-27</td>\n",
       "      <td>0</td>\n",
       "      <td>FNGFrzMmspm0Y2B27sI30g</td>\n",
       "      <td>4</td>\n",
       "      <td>Hash house a go go!!....This is one of my favo...</td>\n",
       "      <td>0</td>\n",
       "      <td>fqxPn_A8EL1Bvo05KifVEA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3611</th>\n",
       "      <td>446249</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-09-04</td>\n",
       "      <td>0</td>\n",
       "      <td>_Si-YQXMGar_-cDpNTFrrA</td>\n",
       "      <td>3</td>\n",
       "      <td>Very Casual Ambiance. It's comfort food but in...</td>\n",
       "      <td>0</td>\n",
       "      <td>uEX__DrTM7O5B16HhDNEfg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3612</th>\n",
       "      <td>446250</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-05-12</td>\n",
       "      <td>0</td>\n",
       "      <td>SORuCnVEftX63iZW0a4_9g</td>\n",
       "      <td>1</td>\n",
       "      <td>We seen this place on Man Vs Food and was real...</td>\n",
       "      <td>0</td>\n",
       "      <td>8SRATp9rU1dJI-FiR-y7qA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3613</th>\n",
       "      <td>446251</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-16</td>\n",
       "      <td>0</td>\n",
       "      <td>zFl6MxQj7lskiKgcUNcQhw</td>\n",
       "      <td>5</td>\n",
       "      <td>Greatest food and drinks! We got the watermelo...</td>\n",
       "      <td>0</td>\n",
       "      <td>TLA8hkN0QbO3taa1JVw2YQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3614</th>\n",
       "      <td>446252</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-08-28</td>\n",
       "      <td>0</td>\n",
       "      <td>6MYfKr6E5wN1zZ6vXNx1Yw</td>\n",
       "      <td>3</td>\n",
       "      <td>So when you come into the restaurant there are...</td>\n",
       "      <td>0</td>\n",
       "      <td>c9S6nVI4Hw7jcXy-S34YVw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3615</th>\n",
       "      <td>446253</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-03</td>\n",
       "      <td>0</td>\n",
       "      <td>TRte5RVvY7BJoGBj3AVBNw</td>\n",
       "      <td>4</td>\n",
       "      <td>Good burger (very large portion) - the meat ha...</td>\n",
       "      <td>0</td>\n",
       "      <td>PdKwaZczlFHOL267EZff9g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3616</th>\n",
       "      <td>446254</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-09-22</td>\n",
       "      <td>0</td>\n",
       "      <td>AOT_oSaaSPQIZnyt_MZKNw</td>\n",
       "      <td>5</td>\n",
       "      <td>Now this is a kind of place where you have to ...</td>\n",
       "      <td>0</td>\n",
       "      <td>KHo41f2YHLHVj2k8WP9j4A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3617</th>\n",
       "      <td>446255</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2016-06-28</td>\n",
       "      <td>1</td>\n",
       "      <td>OvmaoAGePCbsk7IV0hm_YA</td>\n",
       "      <td>5</td>\n",
       "      <td>This place is great. I got the chicken pot pie...</td>\n",
       "      <td>2</td>\n",
       "      <td>fzN6lp4472n7zoZHI2LDGw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3618</th>\n",
       "      <td>446256</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>0</td>\n",
       "      <td>YKmT3r18DYQ-bdkNp0-Vag</td>\n",
       "      <td>3</td>\n",
       "      <td>This place was PACKED when we showed up at noo...</td>\n",
       "      <td>0</td>\n",
       "      <td>mfWwWKcuQQzpOFkaC1S_rw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3619</th>\n",
       "      <td>446257</td>\n",
       "      <td>Hash House A Go Go</td>\n",
       "      <td>['American (Traditional)', 'Breakfast &amp; Brunch...</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1</td>\n",
       "      <td>2015-09-02</td>\n",
       "      <td>1</td>\n",
       "      <td>7zmd7sCgPpO2YUHAI8sfhA</td>\n",
       "      <td>3</td>\n",
       "      <td>I had the Tractor Combo: flapjack, eggs and ba...</td>\n",
       "      <td>1</td>\n",
       "      <td>VFyRyNDEF3jVKUNI-I7iKA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3620 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index                name  \\\n",
       "0      32737  Hash House A Go Go   \n",
       "1      32738  Hash House A Go Go   \n",
       "2      32739  Hash House A Go Go   \n",
       "3      32740  Hash House A Go Go   \n",
       "4      32741  Hash House A Go Go   \n",
       "5      32742  Hash House A Go Go   \n",
       "6      32743  Hash House A Go Go   \n",
       "7      32744  Hash House A Go Go   \n",
       "8      32745  Hash House A Go Go   \n",
       "9      32746  Hash House A Go Go   \n",
       "10     32747  Hash House A Go Go   \n",
       "11     32748  Hash House A Go Go   \n",
       "12     32749  Hash House A Go Go   \n",
       "13     32750  Hash House A Go Go   \n",
       "14     32751  Hash House A Go Go   \n",
       "15     32752  Hash House A Go Go   \n",
       "16     32753  Hash House A Go Go   \n",
       "17     32754  Hash House A Go Go   \n",
       "18     32755  Hash House A Go Go   \n",
       "19     32756  Hash House A Go Go   \n",
       "20     32757  Hash House A Go Go   \n",
       "21     32758  Hash House A Go Go   \n",
       "22     32759  Hash House A Go Go   \n",
       "23     32760  Hash House A Go Go   \n",
       "24     32761  Hash House A Go Go   \n",
       "25     32762  Hash House A Go Go   \n",
       "26     32763  Hash House A Go Go   \n",
       "27     32764  Hash House A Go Go   \n",
       "28     32765  Hash House A Go Go   \n",
       "29     32766  Hash House A Go Go   \n",
       "...      ...                 ...   \n",
       "3590  446228  Hash House A Go Go   \n",
       "3591  446229  Hash House A Go Go   \n",
       "3592  446230  Hash House A Go Go   \n",
       "3593  446231  Hash House A Go Go   \n",
       "3594  446232  Hash House A Go Go   \n",
       "3595  446233  Hash House A Go Go   \n",
       "3596  446234  Hash House A Go Go   \n",
       "3597  446235  Hash House A Go Go   \n",
       "3598  446236  Hash House A Go Go   \n",
       "3599  446237  Hash House A Go Go   \n",
       "3600  446238  Hash House A Go Go   \n",
       "3601  446239  Hash House A Go Go   \n",
       "3602  446240  Hash House A Go Go   \n",
       "3603  446241  Hash House A Go Go   \n",
       "3604  446242  Hash House A Go Go   \n",
       "3605  446243  Hash House A Go Go   \n",
       "3606  446244  Hash House A Go Go   \n",
       "3607  446245  Hash House A Go Go   \n",
       "3608  446246  Hash House A Go Go   \n",
       "3609  446247  Hash House A Go Go   \n",
       "3610  446248  Hash House A Go Go   \n",
       "3611  446249  Hash House A Go Go   \n",
       "3612  446250  Hash House A Go Go   \n",
       "3613  446251  Hash House A Go Go   \n",
       "3614  446252  Hash House A Go Go   \n",
       "3615  446253  Hash House A Go Go   \n",
       "3616  446254  Hash House A Go Go   \n",
       "3617  446255  Hash House A Go Go   \n",
       "3618  446256  Hash House A Go Go   \n",
       "3619  446257  Hash House A Go Go   \n",
       "\n",
       "                                             categories  avg_stars  cool  \\\n",
       "0     ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "1     ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "2     ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "3     ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "4     ['American (New)', 'Restaurants', 'Breakfast &...        3.5     1   \n",
       "5     ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "6     ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "7     ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "8     ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "9     ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "10    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "11    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "12    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "13    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "14    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "15    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "16    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "17    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "18    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "19    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "20    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "21    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "22    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "23    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "24    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "25    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     1   \n",
       "26    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "27    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "28    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "29    ['American (New)', 'Restaurants', 'Breakfast &...        3.5     0   \n",
       "...                                                 ...        ...   ...   \n",
       "3590  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3591  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3592  ['American (Traditional)', 'Breakfast & Brunch...        3.5     2   \n",
       "3593  ['American (Traditional)', 'Breakfast & Brunch...        3.5     3   \n",
       "3594  ['American (Traditional)', 'Breakfast & Brunch...        3.5     1   \n",
       "3595  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3596  ['American (Traditional)', 'Breakfast & Brunch...        3.5     1   \n",
       "3597  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3598  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3599  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3600  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3601  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3602  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3603  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3604  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3605  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3606  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3607  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3608  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3609  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3610  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3611  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3612  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3613  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3614  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3615  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3616  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3617  ['American (Traditional)', 'Breakfast & Brunch...        3.5     1   \n",
       "3618  ['American (Traditional)', 'Breakfast & Brunch...        3.5     0   \n",
       "3619  ['American (Traditional)', 'Breakfast & Brunch...        3.5     1   \n",
       "\n",
       "            date  funny               review_id  stars  \\\n",
       "0     2016-06-22      0  psGDwACpn7tFmWm36865fA      4   \n",
       "1     2017-06-19      0  ZY0ym6jDPXCnyzyRKSVTHg      4   \n",
       "2     2017-01-13      0  vPFRrO6k6ynH-CgGKJLpPQ      5   \n",
       "3     2015-08-26      0  DOZWVKN2n4CAp7mtkhxiaw      1   \n",
       "4     2017-10-16      0  -UGGkrLKjWMdW2N9l2rb2Q      4   \n",
       "5     2017-06-23      0  VvPH04YYZ8RcOimJdZXU7A      4   \n",
       "6     2017-03-29      0  EdCoN1v8Tv7CtSRe5WLyNg      5   \n",
       "7     2015-07-19      0  uGDn7km6sXBQ8NlAC3chhg      5   \n",
       "8     2017-10-23      0  umgRwr9PbF0xOM8p5H4Waw      2   \n",
       "9     2017-09-30      0  uDf1xM8e9BzwtDwN6rb7IQ      4   \n",
       "10    2016-10-12      0  qQwv5cLJsLHUwcSB6hjVhg      5   \n",
       "11    2016-01-07      0  uEHLHXmBXs6UxZ89Pgck4w      4   \n",
       "12    2016-09-15      0  k4-JIiEMSgaNji5tzwBWCw      3   \n",
       "13    2016-08-19      0  sCj_sNKLHpmNbmaXnJrKaQ      4   \n",
       "14    2017-11-26      0  GyCXSnQnyZM-AutIrnTdnQ      4   \n",
       "15    2017-11-09      0  MMm6XGrpwUYG43vn5Oi_7g      5   \n",
       "16    2016-06-02      0  ACkcFtFSpH8T8TR0T0cRgQ      3   \n",
       "17    2016-07-31      0  qhy64zd1s2MEhsC8Jk3Mpg      4   \n",
       "18    2015-07-18      0  29O5vxvulGLgt9Pf8OlBMA      1   \n",
       "19    2017-04-09      0  lvvg7cyQleKOiAtUBQBwGw      4   \n",
       "20    2017-08-13      0  kzAFyixmaVxq_OVkUzEhEg      4   \n",
       "21    2017-03-26      0  Hml9uVRWpxE_a3Ulaa4UOQ      1   \n",
       "22    2016-04-24      0  71_kOnOLW3rUDZC2m27KJQ      5   \n",
       "23    2017-06-05      0  KI4f3vqYXcKn8KVvfadm-g      5   \n",
       "24    2016-05-28      0  CA8gYlcOOvkB9fn2PjLysw      1   \n",
       "25    2017-03-17      0  Xy0Co_vfxVElRoC3ULtk8A      5   \n",
       "26    2016-04-12      0  i5N6ZXARhEc0X4J0eee6hQ      5   \n",
       "27    2015-09-12      0  kA-4M-jJe1H6Eq5UuFQ5mA      4   \n",
       "28    2017-08-30      0  oRTFazyWTz3DxxGT4nhcdQ      5   \n",
       "29    2015-11-16      0  tikz-MWqWmO4GkgKVBugwg      4   \n",
       "...          ...    ...                     ...    ...   \n",
       "3590  2017-10-25      0  KyZ8g0au1wYUSix9LblTUg      5   \n",
       "3591  2015-12-30      0  yjK-33zpOwnuXH-P9Kugdw      1   \n",
       "3592  2016-04-19      1  EHEQo7c1jQtiKkcsVrGCMQ      5   \n",
       "3593  2017-02-01      1  QaEAZ1SsWdQmo8xFh-cvNw      4   \n",
       "3594  2015-12-04      1  s0EZjSzxY7zLvUHGQ6LZHg      5   \n",
       "3595  2016-07-20      0  TvqqOUAl7tpL_0nNRPV_Nw      4   \n",
       "3596  2015-10-19      1  Y7NlEAjiCTUkqmJGEi4_OQ      1   \n",
       "3597  2017-01-04      0  yeZv5VMIGL-qOFiaJ88-Nw      2   \n",
       "3598  2017-04-02      0  OWtZ0oI-Z0EmJXvp7dfYoQ      4   \n",
       "3599  2016-03-20      0  mMGTTy5GEwlz4Sw3NTDAyg      2   \n",
       "3600  2017-04-22      0  Eliuz50LW8kus7ISGCBrBQ      2   \n",
       "3601  2016-01-03      0  nmrV3fqlxyuYa0Ey7TsmOg      3   \n",
       "3602  2016-04-13      0  7u9p8XhyqNqJ7wMvCwthKA      3   \n",
       "3603  2017-09-24      0  gLLMEdwRB7csDD4MOrNYzQ      1   \n",
       "3604  2017-02-19      0  _6IkhF03vyJ5t3ybIYsDVw      3   \n",
       "3605  2017-11-07      0  2wZ_ZT0Ov_nV8Qg1ze_Mng      3   \n",
       "3606  2015-08-03      0  PifKrfL9fLpBmL5CSfJSag      5   \n",
       "3607  2015-09-19      0  Gzo97HLHvtQqjanqJU6YPg      5   \n",
       "3608  2015-09-10      0  i3YL5DSiJ6t4nURd8SH6ww      3   \n",
       "3609  2016-11-15      0  Y5cDr8IGwYaFS2vHaUk1Aw      4   \n",
       "3610  2015-10-27      0  FNGFrzMmspm0Y2B27sI30g      4   \n",
       "3611  2017-09-04      0  _Si-YQXMGar_-cDpNTFrrA      3   \n",
       "3612  2016-05-12      0  SORuCnVEftX63iZW0a4_9g      1   \n",
       "3613  2016-07-16      0  zFl6MxQj7lskiKgcUNcQhw      5   \n",
       "3614  2017-08-28      0  6MYfKr6E5wN1zZ6vXNx1Yw      3   \n",
       "3615  2017-12-03      0  TRte5RVvY7BJoGBj3AVBNw      4   \n",
       "3616  2015-09-22      0  AOT_oSaaSPQIZnyt_MZKNw      5   \n",
       "3617  2016-06-28      1  OvmaoAGePCbsk7IV0hm_YA      5   \n",
       "3618  2017-12-01      0  YKmT3r18DYQ-bdkNp0-Vag      3   \n",
       "3619  2015-09-02      1  7zmd7sCgPpO2YUHAI8sfhA      3   \n",
       "\n",
       "                                                   text  useful  \\\n",
       "0     There isn't much here for vegetarians, but I h...       0   \n",
       "1     Visiting Las Vegas again, and decided to stop ...       0   \n",
       "2     This place is as crazy as Las Vegas.  The twis...       0   \n",
       "3     I've eaten at Hash House A Go Go on the strip ...       0   \n",
       "4     We were told that this was a good place for br...       0   \n",
       "5     EDC food. The chicken (2 breasts or thighs? Ca...       0   \n",
       "6     We went there after checking Yelp during our t...       0   \n",
       "7     Brunch was amazing! I mean the banana French t...       0   \n",
       "8     I felt the service was not stellar at all. Wai...       0   \n",
       "9     I forget what our dish was called but it was t...       0   \n",
       "10    This place was very very yummy!! The plates ar...       0   \n",
       "11    Food is yummy, they give you a GREAT BIG porti...       1   \n",
       "12    We both had bacon and eggs for breakfast. My w...       0   \n",
       "13    This is like the cheesecake factory of vegas f...       1   \n",
       "14    Been here twice now, but needed the second tri...       0   \n",
       "15    You absolutely must try this place. It is deli...       0   \n",
       "16    Ended up here twice on the last trip to Vegas ...       0   \n",
       "17    The food was so fresh and delicious! Our serve...       0   \n",
       "18    if you are into not being served when it is sl...       0   \n",
       "19    My partner and I decided to eat here because o...       0   \n",
       "20    Loved this location and how it wasn't congeste...       1   \n",
       "21    HORRIBLE SERVICE. Came in around 10 o'clock on...       0   \n",
       "22    So this has always been one of my favorite pla...       1   \n",
       "23    This place is amazing! We were staying at The ...       0   \n",
       "24    The only reason I got food to go from here was...       0   \n",
       "25    You could go fuckin HAM up in here !! We're do...       0   \n",
       "26    Amazing Service, Realistic prices, Large Porti...       0   \n",
       "27    Definitely not for the weak stomach, or faint ...       0   \n",
       "28    This place is delicious! Portions are big and ...       0   \n",
       "29    Ordered the corn beef hash and am completely s...       0   \n",
       "...                                                 ...     ...   \n",
       "3590  Excellent food. Giant portions. We came for br...       0   \n",
       "3591  Got food twice due to circumstances and wouldn...       0   \n",
       "3592  Wow lots of choices and SO much food!  This is...       2   \n",
       "3593  Hash House Agogo..\\nIf you have not tried this...       2   \n",
       "3594  Holy breakfast!\\n\\nThis place truly knows how ...       1   \n",
       "3595  Wow, Great Food and Service! Fantastic Breakfa...       0   \n",
       "3596  Hash House A Go Go is a sickening exercise is ...       2   \n",
       "3597  So this location in the Rio is only open for b...       1   \n",
       "3598  This place is well worth it you truly get your...       0   \n",
       "3599  The food was decent. Service was good. Huge po...       0   \n",
       "3600  i have been a big fan of HH for a long time.  ...       0   \n",
       "3601  Met up with friends for breakfast on 1/2/2016....       0   \n",
       "3602  It was just ok. Coffee is good but the hash (w...       0   \n",
       "3603  It's a place of a lot food but no flavor so di...       0   \n",
       "3604  Our waitress HANA is the besssssssst! She is a...       0   \n",
       "3605  People keep raving about this place but I was ...       0   \n",
       "3606  when i first read reviews of hash house a go g...       0   \n",
       "3607  Huge portions...our waiter looked like The guy...       0   \n",
       "3608  I had the basic breakfast was ok. The eggs wer...       0   \n",
       "3609  The food is good, and the portions are enormou...       0   \n",
       "3610  Hash house a go go!!....This is one of my favo...       0   \n",
       "3611  Very Casual Ambiance. It's comfort food but in...       0   \n",
       "3612  We seen this place on Man Vs Food and was real...       0   \n",
       "3613  Greatest food and drinks! We got the watermelo...       0   \n",
       "3614  So when you come into the restaurant there are...       0   \n",
       "3615  Good burger (very large portion) - the meat ha...       0   \n",
       "3616  Now this is a kind of place where you have to ...       0   \n",
       "3617  This place is great. I got the chicken pot pie...       2   \n",
       "3618  This place was PACKED when we showed up at noo...       0   \n",
       "3619  I had the Tractor Combo: flapjack, eggs and ba...       1   \n",
       "\n",
       "                     user_id  \n",
       "0     Y76nS3L426UCz7N_1pUfUQ  \n",
       "1     SeHCNZeTtVvL1HmKFOLSkQ  \n",
       "2     SvpxzDdYOrrI9ntolyNSxQ  \n",
       "3     Io0qqdu_PyKfkr8d7F19mg  \n",
       "4     JrILFVrSIRIacx2qTy5tiA  \n",
       "5     qyPBg6aUIAM83vbkNJCtSQ  \n",
       "6     PdiutioUdu9q8VhtHdzpVQ  \n",
       "7     Eq_3Wq22Xjw2mxVln-NALw  \n",
       "8     Af2xB-Sfv-r0kdwl_FbGzg  \n",
       "9     QpGBJKgosHPz7MBz95NGbA  \n",
       "10    wJkbfSla1nx2Zdg4O_Ebng  \n",
       "11    ysyvsKXwesbsE2tKZNZXYg  \n",
       "12    v51Ii6NupJ0emzMMlVRDsw  \n",
       "13    DjP2lo5wOngdUBNh1NCvbA  \n",
       "14    -Fb4gB1z0OKuo42NOoX9pQ  \n",
       "15    d2i9c1NKB02hjxjslciYag  \n",
       "16    OIiYnTwm9qODSfOV-HJ_WQ  \n",
       "17    k_lO7ngmIgJmfreKITdbNA  \n",
       "18    Br176HwewDGjppTO5zYWCA  \n",
       "19    qUlxA7qV79Vj0MHoddgcRw  \n",
       "20    SS3sFA9ksCT9bjocM3Wbug  \n",
       "21    VHxLO6QhzOo_4uk7fvABrQ  \n",
       "22    5lhisOo609Mymwf9-4MOlA  \n",
       "23    4UaEmDPikjev0avWU7YCnA  \n",
       "24    AkgO_aR6-DZ9KZgd3P4BYA  \n",
       "25    eofcCttybB9kINM9B2ZTEA  \n",
       "26    4yiDNExj3eQb_8KqKoPFDg  \n",
       "27    3g3HVDsFKaftXKN17v8fHg  \n",
       "28    y0cPGHo3C7B8V8L5nsBw-Q  \n",
       "29    _1nOURpPlpbNYbjqn1dfWg  \n",
       "...                      ...  \n",
       "3590  tDceZUKT7RzyamgNZETKzg  \n",
       "3591  Xef5qBsJI4OkucEiSS1gcg  \n",
       "3592  NQffx45eJaeqhFcMadKUQA  \n",
       "3593  8hbMyONy7kQEVosswobVtQ  \n",
       "3594  zjYg5B443x1vK-GLONLa7w  \n",
       "3595  mQ7Eis5jofSsnCvRkySchA  \n",
       "3596  ntlvfPzc8eglqvk92iDIAw  \n",
       "3597  ajxohdcsKhRGFlEvHZDyTw  \n",
       "3598  4iNP1z6jTIiRHYMKEzWjQw  \n",
       "3599  zz9rIM0FmlWYvAzr6dGwVQ  \n",
       "3600  3xLPeHhf2kD4WHAfkABJIw  \n",
       "3601  JnIhH0aEgMB0cZNLV9ya8g  \n",
       "3602  D5Q_eKLy1yolh9n0drT40A  \n",
       "3603  JfJDvW0OESr2SRizhPNypA  \n",
       "3604  2t0XjHgF9SXwiAoUeDAXsg  \n",
       "3605  XpfS2pcAgOIz6Jlyv9nIcg  \n",
       "3606  TqPVG4ZJM_cPBGI0K8lIUQ  \n",
       "3607  k2Oh3tkq3-FOXo2klkyU0Q  \n",
       "3608  WJPeI1ArWf40JLwfcqhmSA  \n",
       "3609  hIOja3-46E73HgYakqoRuw  \n",
       "3610  fqxPn_A8EL1Bvo05KifVEA  \n",
       "3611  uEX__DrTM7O5B16HhDNEfg  \n",
       "3612  8SRATp9rU1dJI-FiR-y7qA  \n",
       "3613  TLA8hkN0QbO3taa1JVw2YQ  \n",
       "3614  c9S6nVI4Hw7jcXy-S34YVw  \n",
       "3615  PdKwaZczlFHOL267EZff9g  \n",
       "3616  KHo41f2YHLHVj2k8WP9j4A  \n",
       "3617  fzN6lp4472n7zoZHI2LDGw  \n",
       "3618  mfWwWKcuQQzpOFkaC1S_rw  \n",
       "3619  VFyRyNDEF3jVKUNI-I7iKA  \n",
       "\n",
       "[3620 rows x 12 columns]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load business dataset (optional)\n",
    "# Take a look at the most reviewed restaurant's profile \n",
    "df_top_restaurant = df[df['name'] == df_top_restaurant].copy().reset_index()\n",
    "df_top_restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Vectorize the text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3620,)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents_top_restaurant\"\n",
    "documents_top_restaurant = df_top_restaurant['text'].values\n",
    "documents_top_restaurant.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Define target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Again, we look at perfect (5 stars) and imperfect (1-4 stars) rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False,  True, False, False])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top_restaurant['target'] = df_top_restaurant['stars'] == 5\n",
    "target_top_restaurant = df_top_restaurant['target'].values\n",
    "target_top_restaurant[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check the statistic of the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3620, 0.42265193370165743, 0.49398104886716776)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(target_top_restaurant), target_top_restaurant.mean(), target_top_restaurant.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Create training dataset and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: documents_top_restaurant\n",
    "# Y: target\n",
    "# Now split the data to training set 80% and test set 20%\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    documents_top_restaurant,\n",
    "    target_top_restaurant,\n",
    "test_size = 0.2, random_state = 42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Get NLP representation of the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer, and name it vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model with your training data\n",
    "vector_train = vectorizer.fit_transform(X_train).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the vocab of your tfidf\n",
    "words = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to transform the test data\n",
    "vector_test = vectorizer.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the trained model to transform all the data\n",
    "vector_documents_top_restaurant = vectorizer.transform(documents_top_restaurant).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Cluster reviews with KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit k-means clustering on the training vectors and make predictions on all data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=4, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Fit k-means clustering on the train vectors\n",
    "\n",
    "kmeans = KMeans(n_clusters = 4)\n",
    "\n",
    "kmeans.fit(vector_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make predictions on all your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on all data\n",
    "assigned_cluster = kmeans.predict(vector_documents_top_restaurant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspect the centroids and find the top 10 features for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 features for each cluster:\n",
      "0: chicken, waffles, fried, sage, bacon, benedict, good, food, place, huge\n",
      "1: food, minutes, wait, time, just, service, good, took, order, table\n",
      "2: hash, good, breakfast, food, house, eggs, pancake, place, potatoes, huge\n",
      "3: great, food, portions, place, service, huge, wait, good, vegas, amazing\n"
     ]
    }
   ],
   "source": [
    "# Find the top 10 features for each cluster.\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-11:-1]\n",
    "print(\"top 10 features for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(words[i] for i in centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "- Using four clusters, the difference among clusters stands out significantly and each cluster now has an unique topic, shows different aspects that customers care about:\n",
    "    - Cluster 0 is surrounding with the topic of food, like chicken and waffles. \n",
    "    - Cluster 1 is surrounding with the topic of waiting time and service.\n",
    "    - Cluster 2 is relating to the breakfast, like eggs and pancake. \n",
    "    - Cluster 3 is mainly about the taste and nutritional value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print out the rating and review of a random sample of the reviews assigned to each cluster to get a sense of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0:\n",
      "    While being seated, I saw the popular Chicken and Waffles being delivered to a neighboring table. The presentation was so fabulous, I felt I had to try it! It was quite tasty, and HUGE! ( Share if you can, or plan on a 'to go' box if you have somewhere to keep the leftovers!) 4 good sized waffles, bacon strips layered about, and two ample boneless fried chicken breasts on top...YUM! My husband had the steel cut oatmeal with fruit so he could help me with my endeavor, but his turned out to be plentiful, also! The oatmeal had banana, apple, blueberries, and finely chopped mango on top.  The restaurant itself was spacious with a comfortable theme. There are numerous autographed menus on display of current celebrities, sports greats, & political figures...interesting to look at while you wait to get seated. Our only drawback was we felt a bit like a cattle-call as far as service. It was a bit impersonal, & my husband's oatmeal came out significantly earlier than my breakfast. He finally started eating before it got cold, with me just looking around wondering if they had forgotten mine.\n",
      "cluster 1:\n",
      "    My husband and I eat at this HH a Go-Go every time we come to Las Vegas. Their service is never a 5, but this time was just absurd. Immediately we noticed that half of the dining area was closed, for no good reason apparently. People were waiting in line to be seated, while there were clearly tables available on both sides. Okay, whatever. We got seated. It took a good 8 minutes before a busser came and took our drink order which was 2 coffees and 2 glasses of water. We got our coffee within minutes, but no water in sight. When our actual server, Marcy, finally came to take our order we asked for our waters again as well as ordering the original HH Farmhouse Benedict for myself and the Chicken and Waffles for my husband. A few minutes later the same busser returned with a water for me. Just for me though, no water for my husband. Marcy was nearly impossible to track down and when we finally got her attention and asked AGAIN for his water she seemed irritated and said okay water and walked away. My husband's chicken and Waffles arrived at the table BEFORE his water did. Finally we both had our food and were excited to be eating. This is the only reason this review has a second star. The taste was what was expected. However for the portions given, I would have expected more than one egg on my Benedict. Before even 5 minutes passes by, Marcy slams our check on our table and says \"whenever you're ready\". No \"no hurry\", or \"any dessert before I give you your tab?\" none of that. Just a rude deliverance of our bill. I felt like I was being rushed out of the restaurant when all I wanted to do was sit down and enjoy a meal. The table next to us clearly had issues with Marcy as well. My husband and I observed a girl at the table ask Marcy a question when she was right next to her, and Marcy ignored her question and walked away. This experience made me want to never return to HH A Go-Go. Or at least not to this location. And honestly, Marcy needs to work on her Service skills because she is the absolute worst.\n",
      "cluster 2:\n",
      "    How does this place have high reviews? I went here many years ago and never returned until an out of towner wanted to go. I guess if you want to be a gluttonous pig and you like mediocre food then this is the place for you. I shared an entree with my friend and I'm glad about that. Portion sizes are too big and out of control. This restaurant is too much hype in my opinion. I've been to this location and the strip location. The strip location was about one of the only restaurants we could find in the middle of the night, and it brought some real interesting folks. I enjoyed hearing drunken stories more than the food.\n",
      "cluster 3:\n",
      "    Had a great breakfast and a good time.  The wait was long and price was high but what do you expect from a popular quality place in Vegas.\n"
     ]
    }
   ],
   "source": [
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = np.arange(0, vector_documents_top_restaurant.shape[0])[assigned_cluster==i]\n",
    "    sample_reviews = np.random.choice(cluster, 1, replace=False)\n",
    "    print(\"cluster %d:\" % i)\n",
    "    for review in sample_reviews:\n",
    "        print(\"    %s\" % df_top_restaurant.loc[review]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Other user cases of clustering\n",
    "- 4.1 Different distance/similarity metrics for clusterings\n",
    "- 4.2 Cluster restaurants by category information\n",
    "- 4.3 Cluster restaurants by restaurant names\n",
    "- 4.4 Cluster restaurants by tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Different distance/similarity metrics for clusterings\n",
    "\n",
    "#### Q: How do you compare with Cosine distance or Euclidean distance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A:\n",
    "- Cosine takes more computation time in comparison to  Euclidean distance. \n",
    "- While the “correlation” distance measures show a better interpretation of the clustered data. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Cluster restaurants by category information\n",
    "**Note:** a business may have mutiple categories, e.g. a restaurant can have both \"Restaurants\" and \"Korean\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((447033,), dtype('O'))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents\"\n",
    "documents = df['categories'].values\n",
    "documents.shape, documents.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: documents_top_restaurant\n",
    "# Y: target\n",
    "# Now split the data to training set 80% and test set 20%\n",
    "documents_train, documents_test = train_test_split(\n",
    "    documents,test_size = 0.2, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer, and name it vectorizer, choose a reasonable max_features, e.g. 1000\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 500)\n",
    "\n",
    "# Train the model with your training data\n",
    "vectors_train = vectorizer.fit_transform(documents_train).toarray()\n",
    "\n",
    "# Get the vocab of your tfidf\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Use the trained model to transform all the reviews\n",
    "vectors_documents = vectorizer.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit k-means clustering on the training vectors and make predictions on all data\n",
    "\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "\n",
    "kmeans.fit(vectors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on all data\n",
    "assigned_cluster = kmeans.predict(vectors_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 features for each cluster:\n",
      "0: restaurants, food, mexican, chinese, thai, barbeque, asian, seafood, fusion, japanese\n",
      "1: bars, nightlife, sushi, restaurants, japanese, american, wine, new, cocktail, seafood\n",
      "2: pizza, italian, restaurants, sandwiches, wings, chicken, salad, food, seafood, delis\n",
      "3: breakfast, brunch, american, restaurants, traditional, sandwiches, food, new, buffets, diners\n",
      "4: american, traditional, new, burgers, restaurants, food, steakhouses, fast, seafood, southern\n"
     ]
    }
   ],
   "source": [
    "# Find the top 10 features for each cluster.\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-11:-1]\n",
    "print(\"top 10 features for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(words[i] for i in centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "#### Cluster restaurants from their category information, the difference among clusters is significant. Each cluster now has an unique topic, such as Cluster 0 is mainly about Mexican and Chinese, Cluster 1 is Japanese, Cluster 2 is Italian,  Cluster 3 is American breakfast, and Cluster 4 is American(Traditional) in vegas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we defined the most representative restaurant as the one with most review comments in each cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cluster 0 representative restaurant: Gangnam Asian BBQ Dining\n",
      "cluster 1 representative restaurant: Lotus of Siam\n",
      "cluster 2 representative restaurant: Secret Pizza\n",
      "cluster 3 representative restaurant: Hash House A Go Go\n",
      "cluster 4 representative restaurant: Gordon Ramsay BurGR\n"
     ]
    }
   ],
   "source": [
    "for i in range(kmeans.n_clusters):\n",
    "    cluster = df.iloc[assigned_cluster == i]\n",
    "    print(\"cluster %d representative restaurant: %s\" % (i, cluster['name'].value_counts().index[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Cluster restaurants by restaurant names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If we cluster categories from business entities, we are trying to find the similarity between restaurant names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((447033,), dtype('O'))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents\"\n",
    "documents_name = df['name'].values\n",
    "documents_name.shape, documents_name.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: documents_top_restaurant\n",
    "# Y: target\n",
    "# Now split the data to training set 80% and test set 20%\n",
    "documents_name_train, documents_name_test = train_test_split(\n",
    "    documents_name,test_size = 0.2, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer, and name it vectorizer, choose a reasonable max_features, e.g. 1000\n",
    "vectorizer_name = TfidfVectorizer(stop_words = 'english', max_features = 500)\n",
    "\n",
    "# Train the model with your training data\n",
    "vectors_train_name = vectorizer_name.fit_transform(documents_train).toarray()\n",
    "\n",
    "# Get the vocab of your tfidf\n",
    "words_name = vectorizer_name.get_feature_names()\n",
    "\n",
    "# Use the trained model to transform all the reviews\n",
    "vectors_documents_name = vectorizer_name.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit k-means clustering on the training vectors and make predictions on all data\n",
    "\n",
    "kmeans_name = KMeans(n_clusters=5)\n",
    "\n",
    "kmeans_name.fit(vectors_train_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on all data\n",
    "assigned_cluster = kmeans_name.predict(vectors_documents_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 features for each cluster:\n",
      "0: restaurants, food, american, mexican, burgers, chinese, new, traditional, fast, seafood\n",
      "1: japanese, sushi, bars, restaurants, fusion, asian, ramen, noodles, seafood, poke\n",
      "2: bars, nightlife, american, restaurants, wine, new, cocktail, sports, traditional, mexican\n",
      "3: breakfast, brunch, american, restaurants, traditional, sandwiches, food, new, buffets, diners\n",
      "4: pizza, italian, restaurants, sandwiches, wings, salad, chicken, food, seafood, american\n"
     ]
    }
   ],
   "source": [
    "# Find the top 10 features for each cluster.\n",
    "top_n = 10\n",
    "top_centroids = kmeans_name.cluster_centers_.argsort()[:, -1:-(top_n+1):-1]\n",
    "print(\"top 10 features for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(words_name[i] for i in centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We notice the most used business names are very straight forword, telling the major business the entities are running.\n",
    "#### While I don't think these clusters are meaningful in distinguishing each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Cluster restaurants by tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### As we have data \"tip.json\", we can cluster the tips business entities to customers, to see whether different business entities emphasis different aspects of their business. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "file_business, file_checkin, file_review, file_tip, file_user = [\n",
    "    'dataset/business.json',\n",
    "    'dataset/checkin.json',\n",
    "    'dataset/review.json',\n",
    "    'dataset/tip.json',\n",
    "    'dataset/user.json'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>date</th>\n",
       "      <th>likes</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tJRDll5yqpZwehenzE2cSg</td>\n",
       "      <td>2012-07-15</td>\n",
       "      <td>0</td>\n",
       "      <td>Get here early enough to have dinner.</td>\n",
       "      <td>zcTZk7OG8ovAmh_fenH21g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>jH19V2I9fIslnNhDzPmdkA</td>\n",
       "      <td>2015-08-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Great breakfast large portions and friendly wa...</td>\n",
       "      <td>ZcLKXikTHYOnYt5VYRO5sg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dAa0hB2yrnHzVmsCkN4YvQ</td>\n",
       "      <td>2014-06-20</td>\n",
       "      <td>0</td>\n",
       "      <td>Nice place. Great staff.  A fixture in the tow...</td>\n",
       "      <td>oaYhjqBbh18ZhU0bpyzSuw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dAa0hB2yrnHzVmsCkN4YvQ</td>\n",
       "      <td>2016-10-12</td>\n",
       "      <td>0</td>\n",
       "      <td>Happy hour 5-7 Monday - Friday</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ESzO3Av0b1_TzKOiqzbQYQ</td>\n",
       "      <td>2017-01-28</td>\n",
       "      <td>0</td>\n",
       "      <td>Parking is a premium, keep circling, you will ...</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>k7WRPbDd7rztjHcGGkEjlw</td>\n",
       "      <td>2017-02-25</td>\n",
       "      <td>0</td>\n",
       "      <td>Homemade pasta is the best in the area</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>k7WRPbDd7rztjHcGGkEjlw</td>\n",
       "      <td>2017-04-08</td>\n",
       "      <td>0</td>\n",
       "      <td>Excellent service, staff is dressed profession...</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SqW3igh1_Png336VIb5DUA</td>\n",
       "      <td>2016-07-03</td>\n",
       "      <td>0</td>\n",
       "      <td>Come early on Sunday's to avoid the rush</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>KNpcPGqDORDdvtekXd348w</td>\n",
       "      <td>2016-01-07</td>\n",
       "      <td>0</td>\n",
       "      <td>Love their soup!</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>KNpcPGqDORDdvtekXd348w</td>\n",
       "      <td>2016-05-22</td>\n",
       "      <td>0</td>\n",
       "      <td>Soups are fantastic!</td>\n",
       "      <td>ulQ8Nyj7jCUR8M83SUMoRQ</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id        date  likes  \\\n",
       "0  tJRDll5yqpZwehenzE2cSg  2012-07-15      0   \n",
       "1  jH19V2I9fIslnNhDzPmdkA  2015-08-12      0   \n",
       "2  dAa0hB2yrnHzVmsCkN4YvQ  2014-06-20      0   \n",
       "3  dAa0hB2yrnHzVmsCkN4YvQ  2016-10-12      0   \n",
       "4  ESzO3Av0b1_TzKOiqzbQYQ  2017-01-28      0   \n",
       "5  k7WRPbDd7rztjHcGGkEjlw  2017-02-25      0   \n",
       "6  k7WRPbDd7rztjHcGGkEjlw  2017-04-08      0   \n",
       "7  SqW3igh1_Png336VIb5DUA  2016-07-03      0   \n",
       "8  KNpcPGqDORDdvtekXd348w  2016-01-07      0   \n",
       "9  KNpcPGqDORDdvtekXd348w  2016-05-22      0   \n",
       "\n",
       "                                                text                 user_id  \n",
       "0              Get here early enough to have dinner.  zcTZk7OG8ovAmh_fenH21g  \n",
       "1  Great breakfast large portions and friendly wa...  ZcLKXikTHYOnYt5VYRO5sg  \n",
       "2  Nice place. Great staff.  A fixture in the tow...  oaYhjqBbh18ZhU0bpyzSuw  \n",
       "3                     Happy hour 5-7 Monday - Friday  ulQ8Nyj7jCUR8M83SUMoRQ  \n",
       "4  Parking is a premium, keep circling, you will ...  ulQ8Nyj7jCUR8M83SUMoRQ  \n",
       "5             Homemade pasta is the best in the area  ulQ8Nyj7jCUR8M83SUMoRQ  \n",
       "6  Excellent service, staff is dressed profession...  ulQ8Nyj7jCUR8M83SUMoRQ  \n",
       "7           Come early on Sunday's to avoid the rush  ulQ8Nyj7jCUR8M83SUMoRQ  \n",
       "8                                   Love their soup!  ulQ8Nyj7jCUR8M83SUMoRQ  \n",
       "9                               Soups are fantastic!  ulQ8Nyj7jCUR8M83SUMoRQ  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(file_tip) as f:\n",
    "    df_tip = pd.DataFrame(json.loads(line) for line in f)\n",
    "df_tip.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1098325,), dtype('O'))"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents\"\n",
    "documents = df_tip['text'].values\n",
    "documents.shape, documents.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now split the data to training set and test set\n",
    "# Now your data is smaller, you can use a typical \"test_size\", e.g. 0.3-0.7\n",
    "documents_train, documents_test = train_test_split(\n",
    "    documents,\n",
    "test_size = 0.7, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TfidfVectorizer, and name it vectorizer, choose a reasonable max_features, e.g. 1000\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features = 500)\n",
    "\n",
    "# Train the model with your training data\n",
    "vectors_train = vectorizer.fit_transform(documents_train).toarray()\n",
    "\n",
    "# Get the vocab of your tfidf\n",
    "words = vectorizer.get_feature_names()\n",
    "\n",
    "# Use the trained model to transform all the reviews\n",
    "vectors_documents = vectorizer.transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=5, n_init=10, n_jobs=1, precompute_distances='auto',\n",
       "    random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit k-means clustering on the training vectors and make predictions on all data\n",
    "\n",
    "kmeans = KMeans(n_clusters=5)\n",
    "\n",
    "kmeans.fit(vectors_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on all data\n",
    "assigned_cluster = kmeans.predict(vectors_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top 10 features for each cluster:\n",
      "0: great, food, service, place, staff, friendly, love, atmosphere, amazing, prices\n",
      "1: place, love, time, amazing, food, service, try, don, delicious, like\n",
      "2: awesome, food, service, place, great, staff, love, friendly, good, best\n",
      "3: best, town, ve, place, vegas, food, pizza, service, love, hands\n",
      "4: good, food, service, great, place, really, nice, pretty, friendly, prices\n"
     ]
    }
   ],
   "source": [
    "# Find the top 10 features for each cluster.\n",
    "top_n = 10\n",
    "top_centroids = kmeans.cluster_centers_.argsort()[:, -1:-(top_n+1):-1]\n",
    "print(\"top 10 features for each cluster:\")\n",
    "for num, centroid in enumerate(top_centroids):\n",
    "    print(\"%d: %s\" % (num, \", \".join(words[i] for i in centroid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We notice that almost all business entities are using positive words in their tips, thus these clusters are not meaningful in distinguishing each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
